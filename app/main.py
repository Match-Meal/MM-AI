import asyncio
from fastapi import FastAPI, Depends, BackgroundTasks
from fastapi.responses import StreamingResponse
from contextlib import asynccontextmanager
from app.core.ai_model import load_model
from app.routers import vision
from app.schemas.dtos import PeriodFeedbackRequest, RecommendRequest, ChatRequest, MealPlanRequest
from app.services.agent import coach
from app.services.vector_store import tool_store
from fastapi.middleware.cors import CORSMiddleware
from app.services.history_service import history_service
from app.core.database import AsyncSessionLocal
from datetime import datetime, date, timedelta

# --- Helper: Stream & Save ---
async def stream_and_save(generator, user_id: int, ai_type: str, question: str, ref_date=None):
    """
    ì œë„ˆë ˆì´í„°ì˜ ì¶œë ¥ì„ ìŠ¤íŠ¸ë¦¬ë°í•˜ë©´ì„œ, ì™„ë£Œ í›„ DBì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    full_answer = ""
    try:
        async for chunk in generator:
            full_answer += chunk
            yield chunk
    except Exception as e:
        print(f"Streaming Error: {e}")
        full_answer += f"\n[Error] {e}"
        yield f"\n[Error] {e}"
    finally:
        # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ DB ì €ì¥ (ë¹„ë™ê¸° ì„¸ì…˜ ë³„ë„ ìƒì„±)
        if full_answer and user_id:
            print(f"ğŸ’¾ Saving History... User={user_id}, Type={ai_type}")
            async with AsyncSessionLocal() as session:
                await history_service.save_chat_history(
                    session, user_id, ai_type, question, full_answer, ref_date
                )

# 1. ìˆ˜ëª… ì£¼ê¸°(Lifespan) ê´€ë¦¬: ì„œë²„ ì¼œì§ˆ ë•Œ ëª¨ë¸ ë¡œë“œ
@asynccontextmanager
async def lifespan(app: FastAPI):
    print("ğŸš€ MatchMeal AI ì„œë²„ ì‹œì‘ ì¤‘...")
    
    async def initialize_data():
        try:
            print("ğŸ” AI ëª¨ë¸ ë¡œë”© ì‹œë„...")
            load_model()
            print("âœ… AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ (RAG ëª¨ë“œ)")
            
            print("ğŸ’¾ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë°ì´í„° ë¡œë”© ë° ì¸ë±ì‹± ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ)...")
            from app.services.vector_store import food_store
            food_store.load_from_csvs()
            print("âœ… ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¤€ë¹„ ì™„ë£Œ")
            
            print("ğŸ› ï¸ ë„êµ¬ ì¸ë±ì‹± ì¤‘...")
            tool_store.index_tools(coach.all_tools)
            print("âœ… ë„êµ¬ ì¸ë±ì‹± ì™„ë£Œ")
            print("âœ¨ ëª¨ë“  ì´ˆê¸°í™” ì‘ì—…ì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âŒ ë°±ê·¸ë¼ìš´ë“œ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # ì´ˆê¸°í™” ì‘ì—…ì„ ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ë¡œ ì‹œì‘
    init_task = asyncio.create_task(initialize_data())
    
    try:
        print("ï¿½ API ì„œë¹„ìŠ¤ ì‹œì‘ ì¤€ë¹„ ì™„ë£Œ (ì´ˆê¸°í™”ëŠ” ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì§„í–‰ ì¤‘)")
        yield
    except BaseException as b:
        if not isinstance(b, asyncio.CancelledError):
            print(f"âš ï¸ lifespan ì¢…ë£Œ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {type(b).__name__}: {b}")
    finally:
        if not init_task.done():
            init_task.cancel()
        print("ğŸ‘‹ AI ì„œë²„ê°€ ì¢…ë£Œë©ë‹ˆë‹¤.")

# 2. ì•± ìƒì„±
app = FastAPI(
    title="MatchMeal AI Server",
    description="Qwen2.5-VL based Food Analysis API",
    version="1.0.0",
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# 3. ë¼ìš°í„° ë“±ë¡
app.include_router(vision.router)


@app.get("/")
def health_check():
    return {"status": "ok", "msg": "MatchMeal AI Ready"}

# [API 1] ê¸°ê°„ë³„ ì‹ë‹¨ í”¼ë“œë°± -> Heavy Model
@app.post("/ai/period-feedback")
async def period_feedback(req: PeriodFeedbackRequest):
    # Handle missing user profile
    user_data = req.user_profile.model_dump() if req.user_profile else {}
    user_id = req.user_profile.user_id if req.user_profile else 0

    # Handle missing nutrition_stats
    avg_cal = req.nutrition_stats.avg_calories if req.nutrition_stats else 0.0
    total_sod = req.nutrition_stats.total_sodium if req.nutrition_stats else 0.0
    total_sug = req.nutrition_stats.total_sugar if req.nutrition_stats else 0.0
    
    context = f"""
    [ìš”ì²­: ê¸°ê°„ë³„ ì‹ë‹¨ ì •ë°€ ë¶„ì„]
    ê¸°ê°„: {req.period_info.start_date} ~ {req.period_info.end_date} (ì´ {req.period_info.total_days}ì¼)
    ê¸°ë¡ëœ ë¼ë‹ˆ ìˆ˜: {req.period_info.recorded_meals}ë¼
    
    [ì˜ì–‘ í†µê³„]
    - ì¼ í‰ê·  ì¹¼ë¡œë¦¬: {avg_cal:.1f}kcal
    - ê¸°ê°„ ì´ ë‚˜íŠ¸ë¥¨: {total_sod:.1f}mg
    - ê¸°ê°„ ì´ ë‹¹ë¥˜: {total_sug:.1f}g
 
    [ì„­ì·¨í•œ ë©”ë‰´ ëª©ë¡]
    {', '.join(req.menu_list) if req.menu_list else 'ê¸°ë¡ëœ ë©”ë‰´ ì—†ìŒ'}
 
    ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì‹ìŠµê´€ì„ í‰ê°€í•˜ê³  ê°œì„ ì ì„ ì•Œë ¤ì£¼ì„¸ìš”.
    """
    
    # use_fast_model=False (Heavy)
    generator = coach.stream_agent_response(context, user_data, use_fast_model=False)
    
    # Question Text for DB
    q_text = f"ê¸°ê°„ë¶„ì„ ìš”ì²­ ({req.period_info.start_date}~{req.period_info.end_date})"

    return StreamingResponse(
        stream_and_save(generator, user_id, "FEEDBACK", q_text, date.today()),
        media_type="text/plain"
    )

# [API 2] ë©”ë‰´ ì¶”ì²œ -> Heavy Model
@app.post("/ai/recommend")
async def recommend(req: RecommendRequest):
    # Handle defaults
    user_data = req.user_profile.model_dump() if req.user_profile else {}
    user_id = req.user_profile.user_id if req.user_profile else 0
    cur_cal = req.current_intake.calories if req.current_intake else 0
    cur_sod = req.current_intake.sodium if req.current_intake else 0
    cur_sug = req.current_intake.sugar if req.current_intake else 0
    
    context = f"""
    [ìš”ì²­: ë§ì¶¤ ë©”ë‰´ ì¶”ì²œ]
    ì‚¬ìš©ìê°€ ì„ íƒí•œ ë¼ë‹ˆ: {req.meal_type}
    
    [ì˜¤ëŠ˜ í˜„ì¬ê¹Œì§€ ì„­ì·¨ëŸ‰]
    - ì¹¼ë¡œë¦¬: {cur_cal}kcal
    - ë‚˜íŠ¸ë¥¨: {cur_sod}mg
    - ë‹¹ë¥˜: {cur_sug}g
    
    [ì‚¬ìš©ì ì·¨í–¥]
    {', '.join(req.flavors) if req.flavors else "íŠ¹ë³„í•œ ì·¨í–¥ ì—†ìŒ"}
    
    ì‚¬ìš©ìì˜ í”„ë¡œí•„(ì§ˆë³‘, ì•Œë ˆë¥´ê¸°)ê³¼ ì˜¤ëŠ˜ ì„­ì·¨ëŸ‰ì„ ê³ ë ¤í•˜ì—¬,
    ë¶€ì¡±í•œ ì˜ì–‘ì†ŒëŠ” ì±„ìš°ê³  ê³¼ì‰ëœ ì˜ì–‘ì†ŒëŠ” í”¼í•  ìˆ˜ ìˆëŠ” ë©”ë‰´ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.
    """
    
    # use_fast_model=False (Heavy)
    generator = coach.stream_agent_response(context, user_data, flavors=req.flavors, use_fast_model=False)
    
    q_text = f"ë©”ë‰´ ì¶”ì²œ ({req.meal_type}, {', '.join(req.flavors)})"

    return StreamingResponse(
        stream_and_save(generator, user_id, "RECOMMENDATION", q_text, date.today()),
        media_type="text/plain"
    )

# [API New] ê¸°ê°„ë³„ ì‹ë‹¨ ì¶”ì²œ -> Heavy Model
@app.post("/ai/meal-plan")
async def meal_plan(req: MealPlanRequest):
    # Handle user profile
    user_data = req.user_profile.model_dump() if req.user_profile else {}
    user_id = req.user_profile.user_id if req.user_profile else 0

    context = f"""
    [ìš”ì²­: ë§ì¶¤ ì‹ë‹¨í‘œ ìƒì„±]
    ê¸°ê°„: {req.period_info.start_date} ~ {req.period_info.end_date} (ì´ {req.period_info.total_days}ì¼)
    
    [ì‚¬ìš©ì ì·¨í–¥]
    {', '.join(req.flavors) if req.flavors else "íŠ¹ë³„í•œ ì·¨í–¥ ì—†ìŒ"}
    
    ìœ„ ê¸°ê°„ ë™ì•ˆ ì‚¬ìš©ìê°€ ì‹¤ì²œí•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ ì‹ë‹¨í‘œë¥¼ ì§œì£¼ì„¸ìš”.
    - **ë„êµ¬ ì‚¬ìš© í•„ìˆ˜:** `recommend_food_from_db` ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° ë¼ë‹ˆì— ì í•©í•œ ë©”ë‰´ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”.
    - **êµ¬ì„±:** ì•„ì¹¨, ì ì‹¬, ì €ë… ë©”ë‰´ì™€ ì¹¼ë¡œë¦¬ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
    - **í˜•ì‹:** ë‚ ì§œë³„ë¡œ êµ¬ë¶„í•˜ì—¬ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥í•´ì£¼ì„¸ìš”. (ë°˜ë“œì‹œ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”)
    """
    
    # use_fast_model=False (Heavy)
    generator = coach.stream_agent_response(context, user_data, flavors=req.flavors, use_fast_model=False)
    
    q_text = f"ì‹ë‹¨í‘œ ìƒì„± ({req.period_info.start_date}~{req.period_info.end_date})"

    return StreamingResponse(
        stream_and_save(generator, user_id, "MEAL_PLAN", q_text, date.today()),
        media_type="text/plain"
    )

# [API 3] ì¼ë°˜ ëŒ€í™” (íˆìŠ¤í† ë¦¬ í¬í•¨) -> Fast Model
@app.post("/ai/chat")
async def chat(req: ChatRequest):
    # Handle user profile
    user_data = req.user_profile.model_dump() if req.user_profile else {}
    user_id = req.user_profile.user_id if req.user_profile else 0

    # use_fast_model=True (Fast)
    generator = coach.stream_agent_response(req.message, user_data, history=req.history, use_fast_model=True, persona=req.persona)
    
    return StreamingResponse(
        stream_and_save(generator, user_id, "CHAT", req.message, date.today()),
        media_type="text/plain"
    )

@app.get("/ai/history/{user_id}")
async def get_history(user_id: int):
    """
    ì‚¬ìš©ìì˜ AI ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ DBì—ì„œ ì¡°íšŒí•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    async with AsyncSessionLocal() as session:
        return {"data": await history_service.get_chat_history(session, user_id)}