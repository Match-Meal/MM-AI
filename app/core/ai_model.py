import os
import torch
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
from peft import PeftModel  # â˜… ì¶”ê°€ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬

# ì „ì—­ ë³€ìˆ˜ (ì‹±ê¸€í†¤ íŒ¨í„´)
_model = None
_processor = None
_device = "cpu"

def get_device_and_dtype():
    """
    í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ì»´í“¨í„°ì˜ í•˜ë“œì›¨ì–´ë¥¼ ê°ì§€í•˜ì—¬ 
    ìµœì ì˜ ì¥ì¹˜(device)ì™€ ë°ì´í„° íƒ€ì…(dtype)ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # 1ìˆœìœ„: NVIDIA GPU (CUDA) - Windows/Linux
    if torch.cuda.is_available():
        print("âœ… í•˜ë“œì›¨ì–´ ê°ì§€: NVIDIA GPU (CUDA)")
        return "cuda", torch.float16
    
    # 2ìˆœìœ„: Apple Silicon (MPS) - Mac M1/M2/M3
    elif torch.backends.mps.is_available():
        print("âœ… í•˜ë“œì›¨ì–´ ê°ì§€: Apple Silicon (MPS)")
        return "mps", torch.float16
    
    # 3ìˆœìœ„: CPU (Fallback) - GPUê°€ ì—†ëŠ” ì„œë²„ ë“±
    else:
        print("âš ï¸ í•˜ë“œì›¨ì–´ ê°ì§€: GPU ì—†ìŒ (CPU ì‚¬ìš©)")
        print("   -> CPUëŠ” ì†ë„ê°€ ëŠë¦¬ë©°, í˜¸í™˜ì„±ì„ ìœ„í•´ FP32ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return "cpu", torch.float32

def load_model():
    """ì„œë²„ ì‹œì‘ ì‹œ AI ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    global _model, _processor, _device
    
    # 1. í™˜ê²½ ê°ì§€
    device, dtype = get_device_and_dtype()
    _device = device
    
    print(f"ğŸ”„ ê¸°ë³¸ Qwen ëª¨ë¸ ë¡œë”© ì¤‘... (Target: {device.upper()})")
    
    try:
        # 1. ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ (ì¸í„°ë„·ì—ì„œ ë‹¤ìš´ë¡œë“œ or ìºì‹œ ì‚¬ìš©)
        base_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            "Qwen/Qwen2.5-VL-3B-Instruct",
            torch_dtype=dtype,
            # Mac(MPS)ì—ì„œëŠ” device_map="auto"ê°€ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆì–´ ìˆ˜ë™ ì´ë™ ì¶”ì²œ
            device_map=None 
        )
        
        # 2. â˜… í•µì‹¬! ìš°ë¦¬ê°€ ë§Œë“  ì–´ëŒ‘í„°(LoRA) ì¥ì°©
        # ê²½ë¡œ: í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ (./models/food_adapter)
        adapter_path = os.path.join(os.getcwd(), "models", "food_adapter_v1.0")
        
        if os.path.exists(adapter_path):
            print(f"ğŸ§© í•™ìŠµëœ ì–´ëŒ‘í„° í•©ì²´ ì¤‘... ({adapter_path})")
            _model = PeftModel.from_pretrained(
                base_model, 
                adapter_path,
                torch_dtype=dtype
            )
        else:
            print(f"âš ï¸ ê²½ê³ : ì–´ëŒ‘í„° í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤! ({adapter_path})")
            print("   -> ê¸°ë³¸ ëª¨ë¸ë¡œë§Œ ë™ì‘í•©ë‹ˆë‹¤.")
            _model = base_model

        # 3. ëª¨ë¸ì„ ì¥ì¹˜(MPS/GPU)ë¡œ ì´ë™
        _model.to(device)
        _model.eval() # ì¶”ë¡  ëª¨ë“œ ì „í™˜

        # 4. í”„ë¡œì„¸ì„œ ë¡œë“œ
        _processor = AutoProcessor.from_pretrained("Qwen/Qwen2.5-VL-3B-Instruct")
        
        print("âœ… AI ëª¨ë¸(LoRA) ë¡œë”© ì™„ë£Œ! ì¤€ë¹„ ë!")
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        raise e

def get_model_instance():
    """ì„œë¹„ìŠ¤ ê³„ì¸µì—ì„œ ëª¨ë¸ì„ í˜¸ì¶œí•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤."""
    if _model is None or _processor is None:
        raise RuntimeError("AI ëª¨ë¸ì´ ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„œë²„ ì‹¤í–‰ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    return _model, _processor, _device