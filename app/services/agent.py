import os
from langchain_openai import ChatOpenAI
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv

from app.services.tools import (
    analyze_health_and_nutrition, 
    recommend_food_from_db,
    calculate_exercise_burn,
    compare_foods,
    generate_shopping_list,
    recommend_seasonal_food,
    recommend_food_for_symptom,
    get_recipe_procedure,
    check_food_compatibility,
    calculate_maintenance_calories,
    suggest_healthy_alternative,
    calculate_water_needs,
    recommend_snack,
    analyze_nutrient_deficiency
)
from app.services.tool_selector import tool_selector

load_dotenv()

import uuid
from typing import Any, List, Optional
from langchain_core.callbacks import CallbackManagerForLLMRun
from langchain_core.outputs import ChatResult

class SanitizedChatOpenAI(ChatOpenAI):
    def _generate(
        self,
        messages: List[Any],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        # ë¶€ëª¨ í´ë˜ìŠ¤ì˜ _generate í˜¸ì¶œ
        chat_result = super()._generate(messages, stop, run_manager, **kwargs)
        
        # ê²°ê³¼ ê²€ì‚¬ ë° ìˆ˜ì •
        if chat_result.generations:
            for generation in chat_result.generations:
                message = generation.message
                
                # 1. message.tool_calls ìˆ˜ì • (LangChain ë‚´ë¶€ìš©)
                if hasattr(message, 'tool_calls') and message.tool_calls:
                    for tool_call in message.tool_calls:
                        if not tool_call.get('id'):
                            new_id = f"call_{str(uuid.uuid4()).replace('-', '')[:24]}"
                            tool_call['id'] = new_id
                            # print(f"âš ï¸ [SanitizedChatOpenAI] Fixed missing tool_call_id (Prop): {new_id}")

                # 2. message.additional_kwargs['tool_calls'] ìˆ˜ì • (OpenAI ì „ì†¡ìš©)
                if hasattr(message, 'additional_kwargs'):
                    raw_tool_calls = message.additional_kwargs.get('tool_calls', [])
                    if raw_tool_calls:
                        for raw_tc in raw_tool_calls:
                            if not raw_tc.get('id'):
                                new_id = f"call_{str(uuid.uuid4()).replace('-', '')[:24]}"
                                raw_tc['id'] = new_id
                                # print(f"âš ï¸ [SanitizedChatOpenAI] Fixed missing tool_call_id (Raw): {new_id}")
        
        return chat_result

    async def _agenerate(
        self,
        messages: List[Any],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        # ë¹„ë™ê¸° í˜¸ì¶œì— ëŒ€í•´ì„œë„ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
        chat_result = await super()._agenerate(messages, stop, run_manager, **kwargs)
        
        if chat_result.generations:
            for generation in chat_result.generations:
                message = generation.message
                
                # 1. message.tool_calls ìˆ˜ì •
                if hasattr(message, 'tool_calls') and message.tool_calls:
                    for tool_call in message.tool_calls:
                        if not tool_call.get('id'):
                            new_id = f"call_{str(uuid.uuid4()).replace('-', '')[:24]}"
                            tool_call['id'] = new_id
                            # print(f"âš ï¸ [SanitizedChatOpenAI] Fixed missing tool_call_id (Async Prop): {new_id}")

                # 2. message.additional_kwargs['tool_calls'] ìˆ˜ì •
                if hasattr(message, 'additional_kwargs'):
                    raw_tool_calls = message.additional_kwargs.get('tool_calls', [])
                    if raw_tool_calls:
                        for raw_tc in raw_tool_calls:
                            if not raw_tc.get('id'):
                                new_id = f"call_{str(uuid.uuid4()).replace('-', '')[:24]}"
                                raw_tc['id'] = new_id
                                # print(f"âš ï¸ [SanitizedChatOpenAI] Fixed missing tool_call_id (Async Raw): {new_id}")
        
        return chat_result

class MatchMealCoach:
    def __init__(self):
        # 1. Fast LLM (Tool Selection, Chat) -> SanitizedChatOpenAI ì ìš©
        self.fast_llm = SanitizedChatOpenAI(
            model="gpt-4.1-mini",
            temperature=1,
            api_key=os.getenv("OPENAI_API_KEY"),
            base_url=os.getenv("OPENAI_API_BASE")
        )

        # 2. Heavy LLM (Complex Reasoning) -> SanitizedChatOpenAI ì ìš©
        self.heavy_llm = SanitizedChatOpenAI(
            model="gpt-5.2", 
            temperature=0.7, # ì•ˆì •ì„±ì„ ìœ„í•´ ì•½ê°„ ë‚®ì¶¤
            api_key=os.getenv("OPENAI_API_KEY"),
            base_url=os.getenv("OPENAI_API_BASE"),
            streaming=True,
            max_tokens=2048 # ì¶©ë¶„í•œ ì¶œë ¥ ê¸¸ì´ë¥¼ ë³´ì¥
        )
        
        # ì „ì²´ ë„êµ¬ ë¦¬ìŠ¤íŠ¸ (Map for Selection)
        self.all_tools = [
            analyze_health_and_nutrition, 
            recommend_food_from_db,
            calculate_exercise_burn,
            compare_foods,
            generate_shopping_list,
            recommend_seasonal_food,
            recommend_food_for_symptom,
            get_recipe_procedure,
            check_food_compatibility,
            calculate_maintenance_calories,
            suggest_healthy_alternative,
            calculate_water_needs,
            recommend_snack,
            analyze_nutrient_deficiency
        ]
        self.tools_map = {tool.name: tool for tool in self.all_tools}
        
        # í˜ë¥´ì†Œë‚˜ ì •ì˜
        self.PERSONA_PROMPTS = {
            "coach": "ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ì˜ì–‘ ì¡°ì–¸ì„ ì œê³µí•˜ëŠ” AI ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìë¥¼ ì¡´ì¤‘í•˜ë©° ê³µì†í•œ ë§íˆ¬(ì¡´ëŒ“ë§)ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.",
            "friend": "30ë…„ ì§€ê¸° 'ì°ì¹œ'ì…ë‹ˆë‹¤. ê²©ì‹ ì—†ì´ í¸ì•ˆí•œ ë°˜ë§ì„ ì‚¬ìš©í•˜ì„¸ìš”. ê±°ì¹œ ë†ë‹´ê³¼ ìœ ë¨¸ë¥¼ ì„ì–´ ëŒ€í™”í•˜ì§€ë§Œ, ì˜ì–‘ ì •ë³´ë§Œí¼ì€ ì¹œêµ¬ë¥¼ ìœ„í•´ ì§„ì‹¬ìœ¼ë¡œ ì •í™•í•˜ê²Œ ì¡°ì–¸í•´ì£¼ì„¸ìš”. (ì˜ˆ: 'ì•¼, ê·¸ì •ë„ ë¨¹ì—ˆìœ¼ë©´ ì´ì œ ì¢€ êµ¶ì–´ë¼', 'ì´ê±´ ëª¸ì— ì•ˆ ì¢‹ìœ¼ë‹ˆê¹Œ ë¨¹ì§€ ë§ˆë¼ ì¢€')"
        }
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (Heavy/Fast ê³µìš© êµ¬ì¡°, ìƒí™©ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
        self.system_prompt_template = """
            ë‹¹ì‹ ì€ 'ëƒ ëƒ ì½”ì¹˜'ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ [ê±´ê°• í”„ë¡œí•„]ê³¼ [ì‹ì‚¬ ê¸°ë¡]ì„ ë¶„ì„í•˜ì—¬, {persona_instruction}

            [ì‚¬ìš©ì í”„ë¡œí•„]
            - ê¸°ë³¸ ì •ë³´: {age}ì„¸ / {gender} / {height}cm / {weight}kg
            - ì‹ ì²´ ì§€ìˆ˜: BMI {bmi} ({bmi_status})
            - ë³´ìœ  ì§ˆí™˜: {diseases}
            - ì•Œë ˆë¥´ê¸°: {allergies}
            - ì‹ì„±/ì·¨í–¥: {flavors}

            ---
            [ë‹µë³€ í˜•ì‹ ê°€ì´ë“œ (í•„ìˆ˜ ì¤€ìˆ˜)]
            1. ëª¨ë“  ë‹µë³€ì€ ì‚¬ìš©ìê°€ í•µì‹¬ì„ ë¨¼ì € íŒŒì•…í•  ìˆ˜ ìˆë„ë¡ **3ì¤„ ìš”ì•½** ì„¹ì…˜ìœ¼ë¡œ ì‹œì‘í•˜ì„¸ìš”.
            2. **3ì¤„ ìš”ì•½**ì´ ëë‚œ í›„ì—ëŠ” ë°˜ë“œì‹œ `---` (ëŒ€ì‹œ 3ê°œ)ë¥¼ ì…ë ¥í•˜ì—¬ ìš”ì•½ê³¼ ìƒì„¸ ë‚´ìš©ì„ êµ¬ë¶„í•´ì£¼ì„¸ìš”.
            3. ìƒì„¸ ë‚´ìš©ì—ì„œëŠ” ë¶„ì„ ê²°ê³¼ì™€ í•¨ê»˜ êµ¬ì²´ì ì¸ ê°œì„  ë°©í–¥ì„ ì œì•ˆí•˜ì„¸ìš”.
            
            [í˜•ì‹ ì˜ˆì‹œ]
            **3ì¤„ ìš”ì•½**
            1. (í•µì‹¬ ë‚´ìš© 1)
            2. (í•µì‹¬ ë‚´ìš© 2)
            3. (í•µì‹¬ ë‚´ìš© 3)

            ---

            (ì´í›„ ìƒì„¸ ë‹µë³€ ì‘ì„±...)
            
            ---
            [ëŒ€í™” ì»¨í…ìŠ¤íŠ¸]
            ìµœê·¼ ëŒ€í™” ë‚´ìš©ì„ í™•ì¸í•˜ê³  íë¦„ì— ë§ê²Œ ë‹µë³€í•˜ì„¸ìš”:
            {history}
            
            ---
            [ì„ë¬´ ê°€ì´ë“œ]
            1. **ë„êµ¬ í™œìš©:** ì œê³µëœ ë„êµ¬ê°€ ìˆë‹¤ë©´ ì ê·¹ í™œìš©í•˜ì„¸ìš”. 
               - íŠ¹íˆ 'ê¸°ê°„ë³„ ì‹ë‹¨ ë¶„ì„' ìš”ì²­ ì‹œì—ëŠ” `analyze_health_and_nutrition` ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ì´ˆëŒ€ì‚¬ëŸ‰ê³¼ ê¶Œì¥ ì„­ì·¨ëŸ‰ì„ ê³„ì‚°í•˜ê³  ë¹„êµ ê²°ê³¼ì— ê¸°ë°˜í•´ ì¡°ì–¸í•˜ì„¸ìš”.
               - ë‚´ë¶€ í•¨ìˆ˜ëª…(ì˜ˆ: analyze_health...)ì€ ì ˆëŒ€ ë‹µë³€ì— ë…¸ì¶œí•˜ì§€ ë§ˆì„¸ìš”.
            2. **ë°ì´í„° ë¶„ì„:** ì„­ì·¨í•œ ë©”ë‰´ ëª©ë¡ê³¼ ì˜ì–‘ í†µê³„ë¥¼ ê¼¼ê¼¼íˆ ì‚´í”¼ì„¸ìš”. ë¶€ì¡±í•˜ê±°ë‚˜ ê³¼ì‰ëœ ì˜ì–‘ì†Œ(ë‚˜íŠ¸ë¥¨, ë‹¹ë¥˜ ë“±)ë¥¼ ì§€ì í•˜ì„¸ìš”.
            3. **ì•ˆì „:** ì‚¬ìš©ìì˜ ì•Œë ˆë¥´ê¸°ë‚˜ ì§ˆë³‘ ì •ë³´ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ê³ ë ¤í•˜ì„¸ìš”.
            4. **ì–´ì¡°:** ì„ íƒëœ í˜ë¥´ì†Œë‚˜ì— ë§ì¶° ì¼ê´€ëœ ë§íˆ¬ë¥¼ ìœ ì§€í•˜ì„¸ìš”.
            """
            
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", self.system_prompt_template),
            ("human", "{input}"),
            ("placeholder", "{agent_scratchpad}"),
        ])

    async def stream_agent_response(self, context_str: str, profile: dict, history: list = [], flavors: list = [], use_fast_model: bool = False, persona: str = "coach"):
        """
        ì œë„ˆë ˆì´í„° í•¨ìˆ˜: ë‹µë³€ì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ yield í•©ë‹ˆë‹¤.
        """
        # History í¬ë§·íŒ…
        history_text = ""
        for h in history:
            role = "ì‚¬ìš©ì" if h.get("role") == "user" else "AI"
            history_text += f"- {role}: {h.get('content')}\n"

        # 0. Partial Prompt ì¤€ë¹„
        persona_instruction = self.PERSONA_PROMPTS.get(persona, self.PERSONA_PROMPTS["coach"])
        
        partial_prompt = self.prompt.partial(
            persona_instruction=persona_instruction,
            age=profile.get('age', 0),
            gender=profile.get('gender', 'Unknown'),
            height=profile.get('height_cm', 170.0),
            weight=profile.get('weight_kg', 60.0),
            bmi=profile.get('bmi', 0.0),
            bmi_status=profile.get('bmi_status', 'Unknown'),
            diseases=profile.get('diseases') or "ì—†ìŒ",
            allergies=profile.get('allergies') or "ì—†ìŒ",
            flavors=", ".join(flavors) if flavors else "ì§€ì • ì•ˆ í•¨",
            history=history_text if history_text else "ì—†ìŒ"
        )

        # 1. ë„êµ¬ ì„ ë³„ (Vector Search + Fast LLM)
        # ëª¨ë“  ìš”ì²­ì— ëŒ€í•´ ë„êµ¬ ì„ ë³„ì„ ìˆ˜í–‰í•´ Context ìµœì í™”
        try:
            selected_tool_names = tool_selector.select_tools(context_str, self.tools_map)
        except Exception as e:
            print(f"Tool Selection Failed: {e}")
            selected_tool_names = []

        selected_tools = [self.tools_map[name] for name in selected_tool_names if name in self.tools_map]
        
        # 2. ëª¨ë¸ ì„ íƒ ë° ì‹¤í–‰ ì „ëµ
        # - use_fast_model=True (Chat): Fast LLM ì‚¬ìš©. ë„êµ¬ê°€ ì—†ìœ¼ë©´ Chainìœ¼ë¡œ, ìˆìœ¼ë©´ Agentë¡œ.
        # - use_fast_model=False (Analysis): Heavy LLM ì‚¬ìš©.
        
        # ë„êµ¬ê°€ ì—†ëŠ”ë° Heavy Modelì„ ì¨ì•¼ í•˜ëŠ” ê²½ìš°? (ì‹¬ì¸µ ì¶”ë¡  í•„ìš” ì‹œ) -> ë¶„ì„ ëª¨ë“œë©´ Heavy Model.
        # ë„êµ¬ê°€ ìˆëŠ”ë° Fast Modelì„ ì¨ì•¼ í•˜ëŠ” ê²½ìš°? (ê°€ë²¼ìš´ ê²€ìƒ‰ ë“±) -> ê°€ëŠ¥.
        
        llm_to_use = self.fast_llm if use_fast_model else self.heavy_llm
        
        # 3. Agent Execution (Streaming)
        if not selected_tools:
            # ë„êµ¬ ì—†ìŒ -> ë‹¨ìˆœ LLM Chain (Streaming)
            # AgentExecutor ì—†ì´ ë°”ë¡œ stream
            print(f"ğŸš€ Running {'FAST' if use_fast_model else 'HEAVY'} Chain (No Tools)")
            chain = partial_prompt | llm_to_use
            async for chunk in chain.astream({"input": context_str}):
                if chunk.content:
                    yield chunk.content
        else:
            # ë„êµ¬ ìˆìŒ -> AgentExecutor (Streaming)
            print(f"ğŸ› ï¸ Running {'FAST' if use_fast_model else 'HEAVY'} Agent with tools: {selected_tool_names}")
            agent = create_tool_calling_agent(llm_to_use, selected_tools, partial_prompt)
            executor = AgentExecutor(
                agent=agent, 
                tools=selected_tools, 
                verbose=True, 
                handle_parsing_errors=True,
                max_iterations=25 # ì‹ë‹¨í‘œ ë“± ë³µì¡í•œ ì‘ì—… ìœ„í•´ ë°˜ë³µ íšŸìˆ˜ ìƒí–¥
            )
            
            try:
                # astream_eventsë¥¼ ì‚¬ìš©í•˜ì—¬ 'on_chat_model_stream' ì´ë²¤íŠ¸ë§Œ í•„í„°ë§í•˜ì—¬ yield
                async for event in executor.astream_events({"input": context_str}, version="v1"):
                    kind = event["event"]
                    
                    # LLMì´ ìŠ¤íŠ¸ë¦¬ë°í•˜ëŠ” í† í° ì¤‘ 'ìµœì¢… ë‹µë³€'ì— í•´ë‹¹í•˜ëŠ” ê²ƒë§Œ ì¶”ì¶œí•´ì•¼ í•¨.
                    if kind == "on_chat_model_stream":
                        # ë°ì´í„° êµ¬ì¡° ì•ˆì „í•˜ê²Œ ì ‘ê·¼
                        data = event.get("data", {})
                        chunk = data.get("chunk")
                        
                        # chunkê°€ ìˆê³ , tool_callsê°€ ì—†ëŠ”(ìˆœìˆ˜ í…ìŠ¤íŠ¸) ê²½ìš°ë§Œ yield
                        if chunk and hasattr(chunk, 'content') and chunk.content:
                            # tool_call_chunksê°€ ìˆìœ¼ë©´(ì¦‰, ë„êµ¬ í˜¸ì¶œ ì¤‘ì´ë©´) ê±´ë„ˆëœ€
                            if hasattr(chunk, 'tool_call_chunks') and chunk.tool_call_chunks:
                                continue
                                
                            yield chunk.content
            except Exception as e:
                # OpenAI 400 Error (Invalid type) ë“±ì„ í¬ì°©í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ì•Œë¦¼
                print(f"Agent Execution Error: {e}")
                yield f"\n\n[ì‹œìŠ¤í…œ ì•Œë¦¼] ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. (Error: {str(e)[:50]}...)"
                # ë¡œê¹… ë˜ëŠ” ì¬ì‹œë„ ë¡œì§ ì¶”ê°€ ê°€ëŠ¥

coach = MatchMealCoach()