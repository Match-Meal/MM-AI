import os
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document
from dotenv import load_dotenv

load_dotenv()

PERSIST_DIRECTORY = "./chroma_db"

class FoodVectorStore:
    def __init__(self):
        # â˜… GMS í™˜ê²½ ì„¤ì • ì ìš©
        self.embedding_function = OpenAIEmbeddings(
            model="text-embedding-3-small",
            api_key=os.getenv("OPENAI_API_KEY"),
            base_url=os.getenv("OPENAI_API_BASE")
        )
        
        self.db = Chroma(
            persist_directory=PERSIST_DIRECTORY,
            embedding_function=self.embedding_function,
            collection_name="food_collection"
        )

    # â˜… CSV íŒŒì¼ ë¡œë“œ ë° ì ì¬
    def load_from_csvs(self):
        # ì´ë¯¸ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸ (ì¤‘ë³µ ì ì¬ ë°©ì§€)
        try:
            if self.db._collection.count() > 0:
                print(f"âœ… ChromaDBì— ì´ë¯¸ {self.db._collection.count()}ê°œì˜ ë°ì´í„°ê°€ ìˆìŠµë‹ˆë‹¤. ì´ˆê¸°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                return
        except Exception as e:
            print(f"âš ï¸ DB í™•ì¸ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")

        import csv
        documents = []
        
        # íŒŒì¼ë³„ ë§¤í•‘ ì„¤ì • [íŒŒì¼ê²½ë¡œ, í—¤ë”í–‰ì¸ë±ìŠ¤, ì¸ì½”ë”©, {í•„ë“œëª…: ì¸ë±ìŠ¤}]
        files_config = [
            {
                "path": "app/400_Food_DB.csv", "header_row": 0, "encoding": "utf-8",
                "map": {"name": 0, "kcal": 2, "carb": 3, "sugar": 4, "fat": 5, "prot": 6, "sodium": 9},
                "desc": "ì¼ë°˜ ìŒì‹"
            },
            {
                "path": "app/50000_Food_DB.csv", "header_row": 3, "encoding": "utf-8", # utf-8ë¡œ ì½íˆëŠ”ì§€ ì¬í™•ì¸ í•„ìš”í•˜ì§€ë§Œ get_columns ì„±ê³µí–ˆìœ¼ë¯€ë¡œ utf-8
                "map": {"name": 5, "kcal": 15, "carb": 21, "sugar": 22, "fat": 20, "prot": 19, "sodium": 45},
                "desc": "ê°€ê³µ ì‹í’ˆ"
            }
        ]

        print("ğŸ”„ CSV ë°ì´í„° ì ì¬ ì‹œì‘...")
        
        for config in files_config:
            fpath = config["path"]
            if not os.path.exists(fpath):
                print(f"âš ï¸ íŒŒì¼ ì—†ìŒ: {fpath}")
                continue
                
            try:
                # 50k DBê°€ utf-8ë¡œ ì„±ê³µí–ˆëŠ”ì§€ í™•ì¸ í•„ìš”. Step 764 ê²°ê³¼ëŠ” utf-8ë¡œ ì„±ê³µí•¨.
                with open(fpath, 'r', encoding=config["encoding"]) as csvfile:
                    reader = csv.reader(csvfile)
                    # í—¤ë” ê±´ë„ˆë›°ê¸°
                    for _ in range(config["header_row"] + 1):
                        next(reader)
                    
                    for row in reader:
                        try:
                            # ì¸ë±ìŠ¤ ì ‘ê·¼ ì•ˆì „ ì¥ì¹˜
                            m = config["map"]
                            if len(row) <= max(m.values()): continue
                            
                            name = row[m["name"]].strip()
                            if not name: continue
                            
                            def safe_float(val):
                                try: return float(val.replace(',', ''))
                                except: return 0.0

                            meta = {
                                "name": name,
                                "calories": safe_float(row[m["kcal"]]),
                                "carbohydrate": safe_float(row[m["carb"]]),
                                "sugar": safe_float(row[m["sugar"]]),
                                "fat": safe_float(row[m["fat"]]),
                                "protein": safe_float(row[m["prot"]]),
                                "sodium": safe_float(row[m["sodium"]])
                            }
                            
                            # ê²€ìƒ‰ìš© í…ìŠ¤íŠ¸ ìƒì„±
                            content = (f"ìŒì‹ëª…: {name}, ì¹¼ë¡œë¦¬: {meta['calories']}kcal, "
                                       f"íƒ„ìˆ˜: {meta['carbohydrate']}g, ë‹¨ë°±: {meta['protein']}g, "
                                       f"ì§€ë°©: {meta['fat']}g, ë‹¹ë¥˜: {meta['sugar']}g")
                                       
                            documents.append(Document(page_content=content, metadata=meta))
                            
                        except Exception as e:
                            continue # ê°œë³„ í–‰ ì˜¤ë¥˜ ë¬´ì‹œ
                            
            except Exception as e:
                print(f"âŒ {fpath} ë¡œë“œ ì‹¤íŒ¨: {e}")

        if documents:
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì¶”ê°€ (ë„ˆë¬´ ë§ìœ¼ë©´ ì—ëŸ¬ ê°€ëŠ¥ì„±)
            batch_size = 100
            for i in range(0, len(documents), batch_size):
                batch = documents[i:i+batch_size]
                self.db.add_documents(batch)
                print(f"   -> {i+len(batch)} / {len(documents)} ì €ì¥ ì™„ë£Œ")
            print("âœ… ëª¨ë“  ë°ì´í„° ì ì¬ ì™„ë£Œ!")
        else:
            print("âš ï¸ ì ì¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # ë°ì´í„° ì ì¬ (ìˆ˜ë™ ì¶”ê°€ìš©)
    def add_foods(self, food_list: list):
        documents = []
        for food in food_list:
            content = f"ìŒì‹ëª…: {food['name']}, ì¹´í…Œê³ ë¦¬: {food.get('category','')}, íŠ¹ì§•: {food.get('desc','')}"
            meta = {
                "name": food['name'],
                "calories": float(food['calories']),
                "protein": float(food['protein']),
                "fat": float(food['fat']),
                "carbohydrate": float(food['carbohydrate']),
                "sodium": float(food.get('sodium', 0)),
                "sugar": float(food.get('sugar', 0))
            }
            documents.append(Document(page_content=content, metadata=meta))
        
        if documents:
            self.db.add_documents(documents)

    # â˜… ê²€ìƒ‰ (í•„í„° ê¸°ëŠ¥ í¬í•¨)
    def search_food(self, query: str, k=5, filter=None):
        try:
            # ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ê²€ìƒ‰ ìƒëµ (ì—ëŸ¬ ë°©ì§€)
            if self.db._collection.count() == 0: return []
            
            if filter:
                return self.db.similarity_search(query, k=k, filter=filter)
            return self.db.similarity_search(query, k=k)
        except Exception as e:
            print(f"Food Search Error: {e}")
            return []

class ToolVectorStore:
    def __init__(self):
        self.embedding_function = OpenAIEmbeddings(
            model="text-embedding-3-small",
            api_key=os.getenv("OPENAI_API_KEY"),
            base_url=os.getenv("OPENAI_API_BASE")
        )
        
        self.db = Chroma(
            persist_directory=PERSIST_DIRECTORY,
            embedding_function=self.embedding_function,
            collection_name="tool_collection"
        )

    def index_tools(self, tools: list):
        """LangChain ë„êµ¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ë²¡í„° DBì— ì €ì¥í•©ë‹ˆë‹¤."""
        # ê¸°ì¡´ ë°ì´í„° í™•ì¸ (ê°„ë‹¨í•˜ê²Œ ì´ë¦„ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬í•˜ê±°ë‚˜, ë§¤ë²ˆ ë®ì–´ì“°ê¸°)
        # ì—¬ê¸°ì„œëŠ” ë§¤ë²ˆ ì´ˆê¸°í™” í›„ ë‹¤ì‹œ ì €ì¥í•˜ëŠ” ë°©ì‹ì´ ì•ˆì „ (ë„êµ¬ ì„¤ëª… ë³€ê²½ ë°˜ì˜)
        
        # ì»¬ë ‰ì…˜ ì´ˆê¸°í™”ê°€ ê¹Œë‹¤ë¡œìš°ë¯€ë¡œ, ê°„ë‹¨íˆ ëª¨ë“  ë„êµ¬ë¥¼ ê°€ì ¸ì™€ì„œ ì´ë¦„ ë¹„êµ?
        # ë˜ëŠ” ê·¸ëƒ¥ ì¤‘ë³µ ê°ì˜¤í•˜ê³  ì—…ë°ì´íŠ¸?
        # Chromaì˜ add_documentsëŠ” IDë¥¼ ì§€ì •í•˜ë©´ ì—…ë°ì´íŠ¸ê°€ ë¨.
        
        documents = []
        for tool in tools:
            # ë„êµ¬ ì´ë¦„ê³¼ ì„¤ëª…ì„ ì €ì¥
            content = f"ë„êµ¬ ì´ë¦„: {tool.name}\nì„¤ëª…: {tool.description}"
            meta = {"name": tool.name}
            # IDëŠ” ë„êµ¬ ì´ë¦„ìœ¼ë¡œ ê³ ì •í•˜ì—¬ ì¤‘ë³µ ì ì¬ ë°©ì§€/ì—…ë°ì´íŠ¸
            documents.append(Document(page_content=content, metadata=meta, id=tool.name))
            
        if documents:
            # IDs list
            ids = [doc.id for doc in documents]
            # ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ì§€ ì•Šê³  upsert(addëŠ” id ìˆìœ¼ë©´ ì—ëŸ¬ë‚  ìˆ˜ ìˆìŒ, Chroma ìµœì‹ ì€ upsert ì§€ì› í™•ì¸ í•„ìš”)
            # Langchain Chroma wrapper: add_documents usually adds. 
            # safe approach: delete and add, or use specific update method.
            # let's try add_documents with ids. If langchain chroma doesn't support upsert by default, we might get dupes if ids not used.
            # Actually, Langchain Chroma `add_documents` usually generates distinct IDs if not provided.
            # If we provide IDs, it might error if exists.
            
            # Resetting collection for tools is safer as tools are few.
            try:
                # This is a bit hacky, but effective for small toolsets
                existing_ids = self.db.get()['ids']
                if existing_ids:
                    self.db.delete(ids=existing_ids)
            except:
                pass
                
            self.db.add_documents(documents)
            print(f"âœ… {len(documents)}ê°œì˜ ë„êµ¬ê°€ ì¸ë±ì‹±ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def search_tools(self, query: str, k=3):
        return self.db.similarity_search(query, k=k)

    def all_tools_docs(self):
        """ì €ì¥ëœ ëª¨ë“  ë„êµ¬ ë¬¸ì„œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        res = self.db.get()
        docs = []
        if not res or not res['ids']: return []
        for i in range(len(res['ids'])):
            docs.append(Document(
                page_content=res['documents'][i],
                metadata=res['metadatas'][i]
            ))
        return docs

food_store = FoodVectorStore()
tool_store = ToolVectorStore()
